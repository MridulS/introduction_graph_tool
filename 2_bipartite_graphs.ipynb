{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th European Conference On Social Networks\n",
    "## Workshop: Introduction to Python's Graph-Tool\n",
    "# 2. Bipartite Graphs\n",
    "**Authors**: <a href='https://www.gesis.org/person/haiko.lietz'>Haiko Lietz</a>, <a href='https://www.gesis.org/person/marcos.oliveira'>Marcos Oliveira</a>, GESIS - Leibniz Institute for the Social Sciences\n",
    "\n",
    "**Version Date**: 12 September 2019\n",
    "\n",
    "**Description**: This notebook introduces ...\n",
    "\n",
    "**License**: <a href='https://www.gnu.org/licenses/gpl-3.0.en.html'>GNU General Public License 3.0</a>\n",
    "***\n",
    "<img src = 'images/bipartite.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selections = [['a_1', 'f_1'], ['a_1', 'f_2'], ['a_1', 'f_3'], ['a_1', 'f_4'], ['a_1', 'f_5'], ['a_2', 'f_3'], ['a_2', 'f_4'], ['a_2', 'f_5'], ['a_2', 'f_6'], ['a_3', 'f_5'], ['a_3', 'f_6'], ['a_3', 'f_7'], ['a_4', 'f_7'], ['a_4', 'f_8'], ['a_5', 'f_9']]\n",
    "selections = pd.DataFrame(selections, columns=['transaction', 'fact'])\n",
    "selections['weight'] = 1\n",
    "selections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Matrix Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_bipartite(df_selections, projection, norm=True, remove_diagonal=False, sym=False):\n",
    "    # dependencies\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.sparse import csr_matrix, coo_matrix, triu\n",
    "    from sklearn.preprocessing import normalize\n",
    "    # function\n",
    "    if {'transaction', 'fact', 'weight'}.issubset(df_selections.columns):\n",
    "        def extract_vertices(df, *columns):\n",
    "            l = [df[column].unique().tolist() for column in columns]\n",
    "            return list(set(itertools.chain.from_iterable(l)))\n",
    "        transactions = extract_vertices(df_selections, 'transaction')\n",
    "        transactions_id = {value: i for i, value in enumerate(transactions)}\n",
    "        facts = extract_vertices(df_selections, 'fact')\n",
    "        facts_id = {value: i for i, value in enumerate(facts)}\n",
    "        rows = [transactions_id[x] for x in df_selections['transaction'].values]\n",
    "        columns = [facts_id[y] for y in df_selections['fact'].values]\n",
    "        cells = df_selections['weight'].tolist()\n",
    "        G = coo_matrix((cells, (rows, columns))).tocsr()\n",
    "        if projection == 'transactions':\n",
    "            if norm == False:\n",
    "                GT = csr_matrix.transpose(G)\n",
    "                I = G*GT\n",
    "            else:\n",
    "                GN = normalize(G, norm='l1', axis=1)\n",
    "                GNT = csr_matrix.transpose(GN)\n",
    "                I = GN*GNT\n",
    "            transactions = pd.DataFrame(transactions)\n",
    "            transactions.columns = ['transaction']\n",
    "            if remove_diagonal == True:\n",
    "                I = I.tolil()\n",
    "                I.setdiag(0)\n",
    "            if sym == False:\n",
    "                return I.tocsr(), transactions\n",
    "            else:\n",
    "                return triu(I.tocoo()).tocsr(), transactions\n",
    "        if projection == 'facts':\n",
    "            GT = csr_matrix.transpose(G)\n",
    "            if norm == False:\n",
    "                H = GT*G\n",
    "            else:\n",
    "                GN = normalize(G, norm='l1', axis=1)\n",
    "                H = GT*GN\n",
    "            w = pd.Series(np.squeeze(np.array(H.sum(axis=1))))\n",
    "            d = pd.Series(np.array(H.diagonal()))\n",
    "            e = d/w\n",
    "            H_nodiag = H.tolil()\n",
    "            H_nodiag.setdiag(values=0)\n",
    "            k = pd.Series(np.array([len(i) for i in H_nodiag.data.tolist()]))\n",
    "            s = k/w\n",
    "            facts = pd.Series(facts)\n",
    "            facts = pd.concat([facts, k, w, d, e, s], axis=1)\n",
    "            facts.columns = ['fact', 'degree', 'weight', 'self_selection', 'embeddedness', 'sociability']\n",
    "            if remove_diagonal == True:\n",
    "                H = H.tolil()\n",
    "                H.setdiag(0)\n",
    "            if sym == False:\n",
    "                return H.tocsr(), facts\n",
    "            else:\n",
    "                return triu(H.tocoo()).tocsr(), facts\n",
    "    else:\n",
    "        print('Dataframe is not a proper selection table.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I, transactions = project_bipartite(df_selections=selections, projection='transactions', norm=False, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_norm, _ = project_bipartite(df_selections=selections, projection='transactions', norm=True, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(I_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, facts = project_bipartite(df_selections=selections, projection='facts', norm=False, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_norm, facts_norm = project_bipartite(df_selections=selections, projection='facts', norm=True, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(H_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = H_norm.nonzero()[0].tolist()\n",
    "j = H_norm.nonzero()[1].tolist()\n",
    "weight = H_norm.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([i, j, weight]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmp_set_num_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(directed=False)\n",
    "vp_fact = g.new_vertex_property('string')\n",
    "for i in range(0, len(facts_norm)):\n",
    "    v = g.add_vertex()\n",
    "    vp_fact[v] = facts_norm['fact'][i]\n",
    "g.vertex_properties['fact'] = vp_fact\n",
    "ep_weight = g.new_edge_property('double')\n",
    "g.add_edge_list(a, eprops=[ep_weight])\n",
    "g.edge_properties['weight'] = ep_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(g, vertex_text=g.vp.fact, edge_pen_width=prop_to_size(g.ep.weight, mi=1, ma=5), output_size=(200, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Social Network Science: Bipartite Graph of Work Concept Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usages = pd.read_csv('data/sns/usages.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "usages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = pd.read_csv('data/sns/publications.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('data/sns/words.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.merge(left=usages, right=publications[['publication_id', 'publication']], on='publication_id')\n",
    "usages = pd.merge(left=foo, right=words, on='word_id')[['publication', 'word']]\n",
    "usages.columns = ['transaction', 'fact']\n",
    "usages['weight'] = 1\n",
    "usages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(usages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Matrix Projection\n",
    "#### 2.2.1.1. Publication Similarity Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_norm, transactions_norm = project_bipartite(df_selections=usages, projection='transactions', norm=True, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = I_norm.nonzero()[0].tolist()\n",
    "v = I_norm.nonzero()[1].tolist()\n",
    "w = I_norm.data.tolist()\n",
    "a = np.array([u, v, w]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = Graph(directed=False)\n",
    "i_vp_transaction = i.new_vertex_property('string')\n",
    "for Iter in range(0, len(transactions_norm)):\n",
    "    v = i.add_vertex()\n",
    "    i_vp_transaction[v] = transactions_norm['transaction'][Iter]\n",
    "i.vertex_properties['transaction'] = i_vp_transaction\n",
    "i_ep_similarity = i.new_edge_property('double')\n",
    "i.add_edge_list(a, eprops=[i_ep_similarity])\n",
    "i.edge_properties['similarity'] = i_ep_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.num_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graph has too many edges to draw\n",
    "# extract the minimum spanning tree\n",
    "tree = min_spanning_tree(i, weights=i.ep.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get largest hub as root of the tree\n",
    "k = i.get_out_degrees(vs=i.get_vertices())\n",
    "max(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.argmax(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i.get_out_degrees(vs=[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = i.vertex(ix)\n",
    "print(i.vp.transaction[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_tree = GraphView(i, efilt=tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_tree.num_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_tree.num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_tree_pos = radial_tree_layout(i_tree, root=ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(i_tree, pos=i_tree_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.2. Word Co-Usage Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_norm, facts_norm = project_bipartite(df_selections=usages, projection='facts', norm=True, remove_diagonal=True, sym=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = H_norm.nonzero()[0].tolist()\n",
    "y = H_norm.nonzero()[1].tolist()\n",
    "z = H_norm.data.tolist()\n",
    "b = np.array([x, y, z]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = Graph(directed=False)\n",
    "h_vp_fact = h.new_vertex_property('string')\n",
    "h_vp_weight = h.new_vertex_property('double')\n",
    "for Iter in range(0, len(facts_norm)):\n",
    "    v = h.add_vertex()\n",
    "    h_vp_fact[v] = facts_norm['fact'][Iter]\n",
    "    h_vp_weight[v] = facts_norm['weight'][Iter]\n",
    "h.vertex_properties['fact'] = h_vp_fact\n",
    "h.vertex_properties['weight'] = h_vp_weight\n",
    "h_ep_weight = h.new_edge_property('double')\n",
    "h.add_edge_list(b, eprops=[h_ep_weight])\n",
    "h.edge_properties['weight'] = h_ep_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.num_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graph is to large to draw: filter the core\n",
    "# get edge weight histogram\n",
    "Min = min(h.ep.weight.a)\n",
    "Max = max(h.ep.weight.a)\n",
    "counts, bins = edge_hist(h, eprop=h.ep.weight, bins=np.logspace(np.log10(Min), np.log10(Max), 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bins[-len(counts):], counts)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('unbehaved distribution of tie weights')\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel('number of edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter edges\n",
    "minimum_edge_weight = 10\n",
    "h_ep_core_edges = h.ep.weight.a > minimum_edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt = GraphView(h, efilt=h_ep_core_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt.num_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt.num_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the graph still has all vertices\n",
    "# just extract the largest component\n",
    "lcc = label_largest_component(h_efilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt_lcc = GraphView(h_efilt, vfilt=lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(h_efilt_lcc, \n",
    "           #vertex_text=h_efilt_lcc.vp.fact, \n",
    "           #vertex_text_position=0, \n",
    "           vertex_size=prop_to_size(h_efilt_lcc.vp.weight, 1, 50), \n",
    "           edge_pen_width=prop_to_size(h_efilt_lcc.ep.weight, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just show components with a minimum size\n",
    "h_efilt_vp_component = h_efilt.new_vertex_property('int')\n",
    "label_components(h_efilt, vprop=h_efilt_vp_component) # store id of component node belongs to in property map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_component_size = 2\n",
    "c = h_efilt_vp_component.a.tolist()\n",
    "core_components = list(set([x for x in c if c.count(x) >= minimum_component_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt_vp_core_component = h_efilt.new_vertex_property('bool')\n",
    "for v in h_efilt.vertices():\n",
    "    h_efilt_vp_core_component[v] = h_efilt_vp_component[v] in core_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt_vfilt = GraphView(h, vfilt=h_efilt_vp_core_component, efilt=h_ep_core_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_efilt_vfilt.num_vertices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_draw(h_efilt_vfilt, \n",
    "           #vertex_text=h_efilt_vfilt.vp.fact, \n",
    "           #vertex_text_position=0, \n",
    "           vertex_size=prop_to_size(h_efilt_vfilt.vp.weight, 1, 50), \n",
    "           edge_pen_width=prop_to_size(h_efilt_vfilt.ep.weight, 1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Exercise\n",
    "Filtering weak co-selection ties is a good way to extract the core when the tie strength distribution is very skewed - as is the case for the word co-usage network. When the tie strength distribution is not very skewed, it is better to use cohesion-based algorithms like <a href='http://intersci.ss.uci.edu/wiki/index.php/Cohesive_blocking'>k-component decomposition</a> to extract the core. The co-authorship network of Social Network Science is such a \"well-behaved\" graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorships = pd.read_csv('data/sns/authorships.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "# remove papers with exceptionally many authors\n",
    "foo = authorships.groupby('publication_id').size().reset_index(name='num_authors')\n",
    "foo = foo[foo['num_authors'] < (np.mean(foo['num_authors'])+3*np.std(foo['num_authors']))]['publication_id']\n",
    "authorships = pd.merge(left=authorships, right=foo, on='publication_id')\n",
    "authorships.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_csv('data/sns/authors.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.merge(left=authorships, right=publications[['publication_id', 'publication']], on='publication_id')\n",
    "authorships = pd.merge(left=foo, right=authors, on='author_id')[['publication', 'author']]\n",
    "authorships.columns = ['transaction', 'fact']\n",
    "authorships['weight'] = 1\n",
    "authorships.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-component decomposition is not available in graph-tool, but an approximation is: Every k-component is also a k-core but not vice versa. Use <a href=''>kcore_decomposition</a> to extract the core of the co-authorship network of authors that are connected if they write papers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
